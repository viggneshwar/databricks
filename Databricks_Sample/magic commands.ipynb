{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a337f87-34e1-4441-b80c-5ea2ad6daf2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Magic Commands in Databricks\n",
    "Let us understand about creating notebooks & magical commands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a87c764-25b2-4662-b25c-c2bb6218a324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Let us learn first about Magical Commands\n",
    "**Important Magic Commands**\n",
    "- %md: allows you to write markdown text to design the notebook \n",
    "- %run: runs a Python file or a notebook\n",
    "- %sh: executes shell commands on the cluster nodes\n",
    "- %fs: allows you to interact with the Databricks file system (Datalake command)\n",
    "- %sql: allows you to run Spark SQL/HQL queries\n",
    "- %python: switches the notebook context to Python\n",
    "- %pip: allows you to install Python packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc8f120-f9d1-4834-afc7-6a6ca24c21af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**Not Important Magic Commands or We learn few of these where we have Cloud(Azure) dependency**\n",
    "- %scala: switches the notebook context to Scala.\n",
    "- %r: switches the notebook context to R\n",
    "- %lsmagic: lists all the available magic commands\n",
    "- %config: allows you to set configuration options for the notebook\n",
    "- %load: loads the contents of a file into a cell\n",
    "- %who: lists all the variables in the current scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da59d771-3e45-4eb0-9bf4-238bc11f798e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to call a notebook from the current notebook using %run magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4bc3299-a1cd-4fa1-90bf-057e9a21f5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/viggneshwar@gmail.com/databricks/Databricks_Sample/markdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a0a228-cb8c-4043-b562-813a691aa61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a linux commands inside a notebook using %sh magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cca1321-fed7-4acb-93ba-2515d73dd036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l\n",
    "echo \"File List Completed\"\n",
    "head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b2355e-ee13-40d1-b3e9-b646a32bfe55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#######We are going to use Databricks Unity Catalog (We don't know about it yet)\n",
    "to create tables and files under the volume (catalog/schema/volume/folder/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08651095-9f05-4e8c-ac84-c4d98bc504bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.volume_fs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47566dee-a630-421b-95bf-a58cb68e4dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Upload some sample data going into (Catalog -> My Organization -> Workspace -> Default -> Volumes) <br> How to run a DBFS (like Hadoop) FS commands inside a notebook using %fs magic command to copy the uploaded data into some other volume from the uploaded volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb793636-956e-4008-b17f-32493cbfbe67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -1 /Volumes/workspace/default/volume_fs\n",
    "head -1 /Volumes/workspace/default/volume_fs/custs\n",
    "tail -1 /Volumes/workspace/default/volume_fs/custs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418a793f-979f-4375-b8f6-3ebd7fb537f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to use Databricks Unity Catalog (We don't know about it yet) \n",
    "\n",
    "to create tables and files under the volume (catalog/schema/volume/folder/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759b9b58-7bbe-4965-901e-250c0376e591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls \"dbfs:///Volumes/workspace/default/volume_fs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54356e4-8869-42e7-91c4-2d9465547ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp \"dbfs:///Volumes/workspace/default/volume_fs/patients.csv\" \"dbfs:///Volumes/workspace/default/volume_fs/patients_copy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62db90ec-9242-4f7d-b810-900d3b0ed268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Learning for the first time the dbutils, we learn in detail later\n",
    "\n",
    "Rather using fs command, we can use databricks utility command (comprehensive) to copy the data/any other filesystem operations in the DBFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774febdd-7ac9-43e2-81d2-0586a7362dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.cp(\"dbfs:///Volumes/workspace/default/volume_fs/patients.csv\", \"dbfs:///Volumes/workspace/default/volume_fs/patients1.csv\")\n",
    "#dbutils.fs.rm(\"dbfs:///Volumes/workspace/default/volume_fs/patients.csv\")\n",
    "dbutils.fs.ls(\"dbfs:///Volumes/workspace/default/volume_fs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b4f289-5935-47d0-947d-ac7c7ee5121e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Spark SQL/HQL Queries inside a notebook using %sql magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e536f1-d791-476d-8134-33b3f5f3794f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists default.cities(id int,city string);\n",
    "insert into default.cities values(3,'Mumbai'),(4,'Lucknow');\n",
    "select * from cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8ed377-f6c7-421b-9040-4a0d4c1cb795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Spark SQL\n",
    "#spark.sql(\"select * from cities\")\n",
    "#spark.sql(\"select * from cities\").explain(True)\n",
    "#spark.sql(\"select * from cities\").explain(False)\n",
    "#spark.sql(\"select * from cities\").head(1)\n",
    "#spark.sql(\"select * from cities\").display()\n",
    "spark.sql(\"select * from cities\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835c1e88-9c44-499d-8f62-10de62454fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update cities set city='Kolkata' where id=4;\n",
    "select * from cities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6f789c-e795-429b-8cd8-852c319054be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show create table cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5391ce1-9a3c-4f4d-9a4b-4906f576c155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--select * from cities\n",
    "from cities select * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d14249-cc0d-423b-92b0-983840d3092c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Python Program inside a notebook using %python  magic command or by default the cell will be enabled with python interpretter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44e12a7-4894-4cde-b355-4853f7477726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sqrt(a):\n",
    "    return a*a\n",
    "print(\"square root function call \",sqrt(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491b51d2-557e-43e9-8584-c376332f30f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the python magic cell itself, we already have spark session object instantiated,\n",
    "\n",
    "so we can lavishly write spark programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3d0e36-2a55-4a63-9e10-de695e40488a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "#import pyspark.sql.session as sprk\\n\",\n",
    "print(spark)\n",
    "spark1 = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark1)\n",
    "df1 = spark1.read.csv(\"/Volumes/workspace/default/volume_fs/patients.csv\",header=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e876ed74-178f-4c91-b5ce-d11ef2c96667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.where(\"married='Yes'\").write.saveAsTable(\"default.we47_patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94dbb0f2-ea44-4bb7-bf08-297043f498f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1),InPatient from default.we47_patients group by 2\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556f8edb-7090-4740-b36e-0f5601ea14b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"default.we47_patients\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9cae3a-76b4-46e5-a034-89ebf4efe7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to install additional libraries in this current Python Interpreter using %pip magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd8503b-ee9b-40ea-a8a5-6bcedf4e4fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypi"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "magic commands",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
