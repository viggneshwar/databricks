{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032ef88e-5b86-45d6-82e7-30c1dee79fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#File Read & Write Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "094e4e6d-82e0-4fb9-ac9a-594ea76a6a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##A catalog named telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627160b4-fb7a-4fab-b66d-10c1fb2bef0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea559e83-05ea-42fc-86a1-7cbdd9e46277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##A schema landing_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d24e2d-299b-494c-a89b-b4aafa535fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists telecom_catalog_assign.landing_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2be62574-e06e-4904-a6ef-be73f3aee9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##A volume landing_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44590efc-fdfb-4493-aa71-32ccaeaa2930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Create volume if not exists telecom_catalog_assign.landing_zone.landing_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9db6219-8f61-4a29-8b56-99e07f7fde36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using dbutils.fs.mkdirs, create folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7512cd30-adea-46c0-a02f-2e4ab0122803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/')\n",
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/')\n",
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54183737-099b-4ea9-87e6-96afd576d872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Why \"Volume Concept\" Matters in Production PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90428168-a9ec-4df4-97b3-2a40814a9713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##⚡Key Reasons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6311e465-1201-4326-92a0-266b2dbfc821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scalability\n",
    "- PySpark is built for distributed computing. Partitioning data by volume ensures that large datasets are split across executors and nodes.\n",
    "- Without volume-aware design, you risk skew (some nodes overloaded, others idle).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42e335a-9cc7-4326-bfd6-5f0ff0963c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Performance Optimization\n",
    "- Properly sized volumes (partitions) reduce shuffle overhead and improve parallelism.\n",
    "- Example: If you have 1 TB of data, you don’t want 10 partitions (too big) or 10 million partitions (too small). You want a balance based on cluster resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aaf602d-2dbf-4024-a42b-c10025ba29ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Resource Management\n",
    "- Production systems run on shared clusters. Volume-based partitioning ensures memory and CPU are used efficiently.\n",
    "- Avoids out-of-memory errors by controlling how much data each executor processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7534a4d7-b664-4d7a-878f-633016f673f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reliability & Fault Tolerance\n",
    "- Large jobs often fail if not volume-aware. Breaking data into manageable chunks allows retries and recovery without reprocessing the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af7a3c4a-8cc6-405e-a88c-0cb638af3656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Maintainability\n",
    "- Volume concepts (like partitioning by date, region, or business unit) make pipelines easier to debug, rerun, and audit.\n",
    "- Example: If a daily batch fails, you can reprocess only that day’s partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafa25ae-a01d-4399-a0b3-38a4d102e89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Compliance & Governance\n",
    "- In financial or regulated environments (like the ones you’ve worked on), volume-based partitioning aligns with audit requirements — you can isolate and prove lineage for specific data slices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f17135-cf2b-4aa9-b573-5302f1722719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##⚖️ Example in Practice\n",
    "Suppose you’re building a fraud detection pipeline in PySpark:\n",
    "- Raw transactions: 2 TB/day\n",
    "- You partition by date + region → each partition ~50 GB\n",
    "- Spark jobs process partitions in parallel → balanced load across cluster\n",
    "- If one region fails, you rerun only that partition, not the entire 2 TB\n",
    "This is the volume concept: designing around data scale, partitioning, and workload distribution so the system is production-ready.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98e63e4-601e-4421-af9a-047fcd37a6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##✅ In short:\n",
    "We go for the volume concept in PySpark because production systems must be scalable, performant, fault-tolerant, and maintainable under massive data loads. It’s the difference between a demo pipeline and a system that can reliably run every day for years.\n",
    "\n",
    "Would you like me to also break down best practices for choosing partition sizes and strategies (like spark.sql.shuffle.partitions, dynamic partition pruning, etc.)? That’s often the next step when making systems truly production-grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1f0f24-80cb-494b-acd8-17625361647c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Prepare Files and copy to volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4db11c-0c93-47ac-bff5-637e6fee0ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##copy customer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bb92e4-9ce0-461d-b6af-248f1718202f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_customer = '/Workspace/Users/viggneshwar@gmail.com/databricks/Databricks_Sample/DataFiles/Customer.csv'\n",
    "dst_customer = '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv'\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(src_customer)\n",
    "    file_exists_src = True\n",
    "except:\n",
    "    file_exists_src = False\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(dst_customer)\n",
    "    file_exists_dst = True\n",
    "except:\n",
    "    file_exists_dst = False\n",
    "\n",
    "if file_exists_src and not file_exists_dst:\n",
    "    dbutils.fs.cp(src_customer, dst_customer)\n",
    "else:\n",
    "    print(\"Customer File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ebdc8a2-539a-4c0e-8ea8-f84ad100fadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##copy telecom usage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24cba4a-62a3-410e-98d8-16731ca52727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_usage = '/Workspace/Users/viggneshwar@gmail.com/databricks/Databricks_Sample/DataFiles/telecom_usage.csv'\n",
    "dst_usage = '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/telecom_usage.csv'\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(src_usage)\n",
    "    file_exists_src_usage = True\n",
    "except:\n",
    "    file_exists_src_usage = False\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(dst_usage)\n",
    "    file_exists_dst_usage = True\n",
    "except:\n",
    "    file_exists_dst_usage = False\n",
    "\n",
    "if file_exists_src_usage and not file_exists_dst_usage:\n",
    "    dbutils.fs.cp(src_usage, dst_usage)\n",
    "else:\n",
    "    print(\"Usage File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf489bd-db09-4524-922e-8ba391710742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##copy tower usage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8095914-1aeb-47a3-ac2c-0c1e6033d6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_tower = '/Workspace/Users/viggneshwar@gmail.com/databricks/Databricks_Sample/DataFiles/tower_usage.csv'\n",
    "dst_tower = '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_usage.csv'\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(src_tower)\n",
    "    file_exists_src_tower = True\n",
    "except:\n",
    "    file_exists_src_tower = False\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(dst_tower)\n",
    "    file_exists_dst_tower = True\n",
    "except:\n",
    "    file_exists_dst_tower = False\n",
    "\n",
    "if file_exists_src_tower and not file_exists_dst_tower:\n",
    "    dbutils.fs.cp(src_tower, dst_tower,False)\n",
    "else:\n",
    "    print(\"Tower Usage File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123351b2-f923-41d2-a26e-d2129c98a2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Check the files exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fdb9f9-7591-4b02-a04d-2db57cbbe9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_to_check = [\n",
    "    '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv',\n",
    "    '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/telecom_usage.csv',\n",
    "    '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_usage.csv'\n",
    "]\n",
    "\n",
    "copy_status = {}\n",
    "for file_path in files_to_check:\n",
    "    try:\n",
    "        dbutils.fs.head(file_path)\n",
    "        copy_status[file_path] = \"Copied Successfully\"\n",
    "    except:\n",
    "        copy_status[file_path] = \"Copy Failed or File Missing\"\n",
    "\n",
    "display([{\"File\": k, \"Status\": v} for k, v in copy_status.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651d0aed-1232-4836-b0ce-4f819e12e77b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_paths = [\n",
    "    '/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv'\n",
    "]\n",
    "\n",
    "df_customer = spark.read.option(\"recursiveFileLookup\", \"true\").csv(customer_paths)\n",
    "display(df_customer)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5810163310742990,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "File_Read_Write_Operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
