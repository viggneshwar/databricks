{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38637c3b-b6dc-4ec8-803d-8fb6f417301f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765002934800}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- In Databricks notebooks, a SparkSession named spark is automatically created when your cluster starts.\n",
    "- You dont need to write:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "- n Databricks, the sparkContext is already tied to the pre‑initialized spark session.\n",
    "- You dont need to separately declare or initialize it — you can directly call spark.sparkContext.parallelize\n",
    "rdd = spark.sparkContext.parallelize([1,2,3])\n",
    "\"\"\"\n",
    "# Create a DataFrame using the active SparkSession. Use DataFrames and Spark SQL instead of raw RDDs\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [1],        # First row with a single integer value\n",
    "        [2],        # Second row with a single integer value\n",
    "        [3],        # Third row with a single integer value\n",
    "        [4],        # Fourth row with a single integer value\n",
    "        ['test']    # Fifth row with a string value\n",
    "    ],\n",
    "    [\"df value\"]      # Define the column name for the DataFrame as \"value\"\n",
    ")\n",
    "\n",
    "# Display the DataFrame in tabular format (Databricks notebooks render it nicely)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b9324d7-6627-49f2-a9e2-1f2f015c615f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_session_declaration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
